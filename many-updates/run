#!/usr/bin/env bash

set -e

NUM_REPS=125

SCHEMA_NAME=many_updates
BUCKET_NAME=many-updates

OUTPUT_BASE_DIR="results/raw/$SCHEMA"
mkdir -p "$OUTPUT_BASE_DIR"

export HUDI_CONF_DIR=$PWD

for VARIANT in hudi_cow hudi_mor iceberg_cow iceberg_mor delta_no_deletion_vectors; do

    SPARK_CONF='--conf spark.sql.parquet.compression.codec=gzip'

    if [[ "$VARIANT" == 'hudi_cow' ]]; then
        FORMAT=hudi
        TBLPROPERTIES="TBLPROPERTIES ('type' = 'cow')"
    elif [[ "$VARIANT" == 'hudi_mor' ]]; then
        FORMAT=hudi
        TBLPROPERTIES="TBLPROPERTIES ('type' = 'mor')"
    elif [[ "$VARIANT" == 'iceberg_cow' ]]; then
        FORMAT=iceberg
        TBLPROPERTIES="TBLPROPERTIES ('write.delete.mode' = 'copy-on-write', 'write.update.mode' = 'copy-on-write', 'write.merge.mode' = 'copy-on-write', 'commit.manifest-merge.enabled' = false)"
        SPARK_CONF="$SPARK_CONF --conf spark.sql.catalog.spark_catalog.cache-enabled=false"
    elif [[ "$VARIANT" == 'iceberg_mor' ]]; then
        FORMAT=iceberg
        TBLPROPERTIES="TBLPROPERTIES ('write.delete.mode' = 'merge-on-read', 'write.update.mode' = 'merge-on-read', 'write.merge.mode' = 'merge-on-read', 'commit.manifest-merge.enabled' = false)"
        SPARK_CONF="$SPARK_CONF --conf spark.sql.catalog.spark_catalog.cache-enabled=false"
    elif [[ "$VARIANT" == 'delta_no_deletion_vectors' ]]; then
        FORMAT=delta
        TBLPROPERTIES="TBLPROPERTIES ('delta.checkpointInterval' = 2000000000)"
    else
        FORMAT="$VARIANT"
        TBLPROPERTIES=''
    fi

    OUTPUT_DIR="$OUTPUT_BASE_DIR/$VARIANT"
    if [[ -d "$OUTPUT_DIR" ]]; then
        echo "$OUTPUT_DIR already exists"
        exit 1
    fi

    spark-sql-hms-$FORMAT $SPARK_CONF -e "DROP SCHEMA IF EXISTS $SCHEMA_NAME CASCADE;"
    mcli mb -p "minio/$BUCKET_NAME"
    mcli rb --force "minio/$BUCKET_NAME"

    mkdir -p "$OUTPUT_DIR"/{sar,trace_op,trace_select,runtime_op,runtime_select,minio_du,hms,res}

    mcli mb "minio/$BUCKET_NAME"
    spark-sql-hms-$FORMAT $SPARK_CONF -e "$(cat << EOF
CREATE SCHEMA $SCHEMA_NAME LOCATION 's3a://$BUCKET_NAME/';
CREATE TABLE $SCHEMA_NAME.data (key INT, value INT) USING $FORMAT $TBLPROPERTIES;
INSERT INTO $SCHEMA_NAME.data VALUES (1, 0);
EOF
)"

    # Start sar monitoring
    sar -o "$OUTPUT_DIR/sar/benchmark.sar" -A 1 &> /dev/null &
    SAR_PID=$!

    sleep 3

    for REP in $(seq -w 1 "$NUM_REPS"); do

        mcli admin trace minio > "$OUTPUT_DIR/trace_op/$REP" &
        TRACE_PID=$!
        spark-sql-hms-$FORMAT $SPARK_CONF --conf spark.eventLog.enabled=true --conf "spark.eventLog.dir=file://$OUTPUT_DIR/runtime_op" --database "$SCHEMA_NAME" -e "UPDATE data SET value = $((10#$REP)) WHERE key = 1;" &> /dev/null
        kill -SIGINT $TRACE_PID

        mcli admin trace minio > "$OUTPUT_DIR/trace_select/$REP" &
        TRACE_PID=$!
        spark-sql-hms-$FORMAT $SPARK_CONF --conf spark.eventLog.enabled=true --conf "spark.eventLog.dir=file://$OUTPUT_DIR/runtime_select" --database "$SCHEMA_NAME" -e 'SELECT * FROM data;' 2> /dev/null > "$OUTPUT_DIR/res/$REP"
        kill -SIGINT $TRACE_PID

        # Collect MinIO bucket size and number of objects
        s3cmd du "s3://$BUCKET_NAME/" > "$OUTPUT_DIR/minio_du/$REP"

        # Collect HMS SQL dump
        sudo -u postgres pg_dump hms > "$OUTPUT_DIR/hms/$REP"

        echo "$VARIANT: $REP / $NUM_REPS"

    done

    # Stop sar monitoring
    kill -SIGINT $SAR_PID

    spark-sql-hms-$FORMAT $SPARK_CONF -e "DROP SCHEMA $SCHEMA_NAME CASCADE;"
    mcli rb --force "minio/$BUCKET_NAME"

done

unset HUDI_CONF_DIR
